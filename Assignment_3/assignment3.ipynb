{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zu7Emh_WaY4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dT0k9iMN7Qz"
      },
      "source": [
        "# ***Assignment 3***\n",
        "Welcome to Assignment 3! In this assignment you are allowed to work ***individually or in pairs***. It is worth 30 points in total. Exercise 1 is worth 3 points, Exercise 2 is worth 2 points, Exercises 3 and 4 are worth 10 points each, and Exercise 5 is worth 5 points. There is a 5 point minimum for passing this assignment (you need to pass all four assignments to be able to pass the course, see the grade chart on the Canvas course page for more details).\n",
        "\n",
        "Submission details: Your submission should contain two pdf's.\n",
        "\n",
        "1. A pdf version of your filled out colaboratory on Canvas. You can do this by pressing `cmd/ctrl+p` (you know the drill from there).  \n",
        "2. For Exercise 1, you need to hand in your hand-written solutions in a LaTeX pdf. We only accept solutions written in LaTeX, i.e. not Word or any other text editor. We recommend [Overleaf](https://overleaf.com), if you do not already have a favourite LaTeX editor (which is also [provided by KTH](https://intra.kth.se/en/it/programvara-o-system/programvara/installera/download/overleaf/overleaf-1.932755))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp9dXr2ULwzZ"
      },
      "source": [
        "# Contents\n",
        "In this assignment we will cover the following topics (not necessarily in this order):\n",
        "* Derivations of the Importance Weights\n",
        "* Generate and Visualize the Data\n",
        "* Sequential Importance Sampling\n",
        "* Bootstrap Particle Filter\n",
        "* Parameter Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrXaIaQZQTky"
      },
      "source": [
        "# ***1. Derivations of the Importance Weights***\n",
        "\n",
        "***For this exercise, submit your solution in a separate pdf (written in LaTeX) to avoid point deduction***\n",
        "\n",
        "In the lectures, we saw that the importance weights in SMC can be written as\n",
        "\n",
        "$\\begin{equation}\n",
        "w_n = \\frac{p(y_{1:n}|x_{1:n})p(x_{1:n})}{q(x_{1:n})},\n",
        "\\end{equation}$\n",
        "where $x_{1:n} \\sim q(x_{1:n}).$\n",
        "\n",
        "However, as we discussed, it is difficult to sample directly from $q(x_{1:n})$, and difficult to evaluate $w_n$ (when in the form above). Luckily we can make some simplifying assumptions by observing that we are dealing with a hidden Markov model (HMM). Your task is to show, based on these simplifying assumptions that,\n",
        "$$w_n = \\frac{p(y_{1:n}|x_{1:n})p(x_{1:n})}{q(x_{1:n})}=\\alpha(x_{n})w_{n-1},$$\n",
        "where\n",
        "$$x_{n}\\sim q(x_n|x_{n-1})$$\n",
        "$$\\alpha(x_{n}) =\\frac{p(y_{n}|x_{n})p(x_{n}|x_{n-1})}{q(x_{n}|x_{n-1})}$$\n",
        "$$w_{n-1}=\\frac{p(y_{1:n-1}|x_{1:n-1})p(x_{1:n-1})}{q(x_{1:n-1})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOl3cA9-wk9K"
      },
      "source": [
        "## **Hint: take the following steps**\n",
        "\n",
        "**1) Enumerate the simplifying assumptions needed**\n",
        "\n",
        "In the associated lecture we went through a couple of (HMM-related) assumptions and properties that should be useful.\n",
        "\n",
        "**2) Attack the problem term by term**\n",
        "\n",
        "Work with $p(y_{1:n}|x_{1:n})$, $p(x_{1:n})$ and $q(x_{1:n})$ separately.\n",
        "\n",
        "**3) Put the terms together**\n",
        "\n",
        "Once you have expressions for the three separate terms above, put them back into $w_n$ and show that you can re-write it as $$w_n =\\alpha(x_{n})w_{n-1},$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdljF1MsPNli"
      },
      "source": [
        "# ***2. The Stochastic Volatility Model***\n",
        "In this assignment you will work with the stochastic volatililty (SV) model, which was covered in class:\n",
        "\n",
        "$$\\begin{equation*}\n",
        "        p(x_1) = \\mathcal{N}(0, \\frac{\\sigma^2}{1 - \\alpha ^2}),\n",
        "\\end{equation*}$$\n",
        "\n",
        "$$\\begin{equation*}\n",
        "        p(x_n|x_{n-1}) = \\mathcal{N}(\\alpha x_{n-1}, \\sigma^2),\n",
        "\\end{equation*}$$\n",
        "\n",
        "$$\\begin{equation*}\n",
        "        p(y_n|x_n) = \\mathcal{N}(0,  \\beta^2\\exp(x_{n})),\n",
        "\\end{equation*}$$\n",
        "\n",
        "for $n=1,...,T$.\n",
        "\n",
        "Now, let $T=100$, $\\beta=0.64$, $\\alpha=0.91$ and $\\sigma=1$, and generate data according to the SV model. Then visualize $x_{1:T}$ and $y_{1:T}$ in cells below, in a similar manner as plots in the lecture slides. ***Importantly, use correct legends, a title, and x- and y-labels. Failing to include these items will lead to point deduction. This applies for all plots in this assignment.***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG6QlC3tx3Xz"
      },
      "source": [
        "# Visualize the data here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvoSjP96Oxtf"
      },
      "source": [
        "# ***3. Sequential Importance Sampling***\n",
        "\n",
        "Here you will implement the sequential importance sampling (SIS) algorithm in order to  \n",
        "\n",
        "1.   Estimate $x_n$ at each time step $n$.\n",
        "\n",
        "To do so, the expression for the filtering estimate at time $n$ will be useful. Write it in the cell below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWuJmKpk0SBn"
      },
      "source": [
        "*Write the expression for the filtering estimate here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1LQ0uAM1NS9"
      },
      "source": [
        "The filtering estimate above is indeed a pdf. To estimate $x_n$ use the mean of the filtering estimate at each $n$, and refer to it as $\\hat{x}_n$. Report the mean squared error (MSE) between $\\hat{x}_n$ and $x_n$, averaged over all $n$ (i.e., we are expecting a single value)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU8ISumw0m_s"
      },
      "source": [
        "Additionally, you should\n",
        "2.   Visualize the degeneracy of the importance weights using a histogram for $n=2, 5, 50, 100$\n",
        "3.   Visualize the path/trajectory degeneracy by sampling from your SIS approximation of $p(x_{1:T}|y_{1:T})$. Clearly explain how you sample from the distribution (see the lecture slides for hints)!\n",
        "\n",
        "In order to implement the SIS, you will first need to choose a proposal distribution, $q(x_n|x_{n-1})$. For simplicity, let $q(x_n|x_{n-1}) = p(x_n|x_{n-1})$.\n",
        "\n",
        "Start out by using $K=100$ particles.\n",
        "\n",
        "4. How do the MSE scores change when you instead use $K=5$ and $K=500$ particles? Explain your results. **Answer in a text cell after your code cells, and report the MSEs there.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yIkNltJ28p0"
      },
      "source": [
        "# Implement SIS here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGz969YF3VVD"
      },
      "source": [
        "# ***4. Bootstrap Particle Filter***\n",
        "\n",
        "Here you will implement the bootstrap particle filter (BPF). You will do resampling at every time step, $n$.\n",
        "\n",
        "1.   How does the update equation for the importance weights change now that we resample at each $n$?\n",
        "\n",
        "Do the following items using **first multinomial resampling, then stratified resampling** while clearly demonstrating how you implemented the resampling schemes:\n",
        "\n",
        "2.   Estimate $x_n$ at each time step $n$.\n",
        "3.   Visualize the degeneracy of the importance weights using a histogram for $n=2, 5, 50, 100$\n",
        "4.   Visualize the path/trajectory degeneracy by sampling from your BPF approximation of $p(x_{1:T}|y_{1:T})$.\n",
        "\n",
        "In the title of your plots, state which resampling scheme you used to produce the corresponding results. Use $K=100$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwgdfWQU5rWf"
      },
      "source": [
        "# Implement BPF here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODvj9WFo6KKs"
      },
      "source": [
        "# ***5. Parameter Inference using the Marginal Likelihood***\n",
        "In SMC methods, the parameters of the model, $\\theta$, are often unknown. In the SV model above, we assumed that the parameter values were $\\theta=(\\beta, \\alpha, \\sigma^2) = (0.64, 0.91, 1)$, and generated data using these values. Here, you are instead given observations, $y_{1:T}$, generated by some *unknown* parameters and $x_{1:T}$. How does one decide which parameters to use in order to infer the latent variables, $x_{1:T}$?\n",
        "\n",
        "In the next assignment you will infer the parameters in a more sophisticated manner, using the particle Gibbs algorithm. For now you will use brute force to find a viable solution, namely through a grid search. You are to iterate over different combinations of $\\theta$, and for each combination compute the marginal log-likelihood $\\log p(y_{1:T}|\\theta)$ (log for numerical stability).\n",
        "\n",
        "\n",
        "Use the BPF algorithm and resample at every $n$. In the text cell below, type the expression for this the corresponding approximation of $p(y_{1:T}|\\theta)$ (you can find it in the lecture slides):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN-vZ07pazpm"
      },
      "source": [
        "*Type the expression for the approximation of $p(y_{1:T}|\\theta)$ here, given that you use the BPF and do resampling every time step*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWUn747_bO_k"
      },
      "source": [
        "Make a reasonably coarse grid (say, $8\\times8\\times 8$) for $\\theta$ in the interval ($0$, $1$). Use the BPF with Multinomial resampling and $K=100$ to estimate the marginal log-likelihood for each pair of $\\theta$ in the grid. Run the BPF 10 times (each with different random seed) for each parameter combination, and average your estimates.\n",
        "\n",
        "Based on your results, which parameter combination would you choose?\n",
        "\n",
        "List the top 10 parameter combinations and their corresponding likelihood scores. Comment on the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5YOSern6VuK"
      },
      "source": [
        "# load observations.npy from Canvas and do the grid search below"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}